{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd43a121-7fbf-4fbc-a6e7-d8824885201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ISSN_ISBN_Comparison\") \\\n",
    "    .config(\"spark.executor.instances\", \"5\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"<your-spark-image>\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16346e6-3548-4d24-ae6c-738f5766bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the validation DataFrame (issn_isbn.csv)\n",
    "issn_isbn_df = spark.read.option(\"header\", \"true\").csv('/share/smartdata/citations/issn_isbn.csv', inferSchema=True)\n",
    "\n",
    "# Load the input DataFrame (checkpoint_4_processed/*.csv)\n",
    "input_files_df = spark.read.option(\"header\", \"true\").csv('/share/smartdata/citations/checkpoint_4_processed/*.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64df650-b08c-4aac-b3dd-2781d7507313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation successful\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"/share/smartdata/citations/checkpoint_spark_issn\"\n",
    "\n",
    "# Use a left semi join to keep only rows with \"issn/isbn\" that exist in the issn_isbn.csv\n",
    "filtered_df = input_files_df.join(\n",
    "    issn_isbn_df,\n",
    "    input_files_df[\"issn/isbn\"] == issn_isbn_df[\"Identifier\"],\n",
    "    \"left_semi\"\n",
    ")\n",
    "\n",
    "# Save the filtered DataFrame back to a directory (optional)\n",
    "filtered_df.write.option(\"header\", \"true\").csv(output_folder, mode='overwrite')\n",
    "print(\"Operation successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bec12c-51b3-4fc9-91ed-48eab5ff1fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: omid:br/06604017286 pmid:968900\n",
      "Title: Rga (Rodgers) And The HLA Region: Linkage And Associations.\n",
      "Author: C M Giles [omid:ra/066010948469]\n",
      "Venue: Tissue Antigens [omid:br/0614049880 issn:1399-0039 issn:0001-2815]\n",
      "Publication Date: 1976\n",
      "ISSN/ISBN: 0001-2815\n"
     ]
    }
   ],
   "source": [
    "# Get the first row of the DataFrame\n",
    "first_row = df.first()\n",
    "\n",
    "# Print each value from the first row in separate outputs\n",
    "print(\"ID:\", first_row['id'])\n",
    "print(\"Title:\", first_row['title'])\n",
    "print(\"Author:\", first_row['author'])\n",
    "print(\"Venue:\", first_row['venue'])\n",
    "print(\"Publication Date:\", first_row['pub_date'])\n",
    "print(\"ISSN/ISBN:\", first_row['issn/isbn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d67d23-69e8-454f-8e33-5a6aef2b6714",
   "metadata": {},
   "source": [
    "### Ora che ho eliminato tutti i valori per cui i miei issn/isbn non combaciassero, quindi ho solo i venues che mi interessano, conto quante righe ci sono in totale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17f2113-f7fe-4ffe-aec2-7b93b43d9283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================================================> (306 + 8) / 314]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows across all datasets is: 123126818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Directory containing all CSV files\n",
    "data_directory = \"/share/smartdata/citations/checkpoint_spark_issn\"\n",
    "\n",
    "# Read all CSV files in the directory into a single DataFrame\n",
    "all_data_df = spark.read.option(\"header\", \"true\").csv(data_directory)\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "row_count = all_data_df.count()\n",
    "\n",
    "# Print the result\n",
    "print(f\"The total number of rows across all datasets is: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b4f95-4cfb-4e7b-9f48-e9919bc50267",
   "metadata": {},
   "source": [
    "### Eliminiamo i csv vuoti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a149f594-7fd2-4500-b811-a94bc65e5126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty files identified (not deleted): []\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# List all CSV files in the directory using glob\n",
    "files = glob.glob(os.path.join(data_directory, '*.csv'))\n",
    "\n",
    "# Check each file and delete if empty\n",
    "empty_files = []\n",
    "for file_path in files:\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "    if df.count() == 0:  # Check if the DataFrame is empty\n",
    "        empty_files.append(file_path)\n",
    "        # Uncomment the next line to delete the file. Be cautious with deletion operations.\n",
    "        #os.remove(file_path)\n",
    "        print(f\"Deleted empty file: {file_path}\")\n",
    "    #else:\n",
    "        #print(f\"Checked and retained: {file_path}\")\n",
    "\n",
    "# Optionally, print all identified empty files (for review before actual deletion)\n",
    "print(\"Empty files identified (not deleted):\", empty_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3f82a-b919-468a-bd44-b57f93e474e1",
   "metadata": {},
   "source": [
    "### Modifico il formato di pub_date rendendolo uniforme per tutti e lo salvo in UPDATE_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e917d976-515f-442e-b7fe-08c61374a575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "data_directory = \"/share/smartdata/citations/checkpoint_spark_issn\"\n",
    "# Read all CSV files in the directory into a single DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(data_directory + \"/*.csv\")\n",
    "\n",
    "# Extract the year from the 'pub_date' and overwrite the 'pub_date' column\n",
    "df = df.withColumn(\"pub_date\", regexp_extract(col(\"pub_date\"), r\"^(\\d{4})\", 1))\n",
    "\n",
    "# Specify the output directory for the modified data\n",
    "output_directory = \"/share/smartdata/citations/update_date\"\n",
    "df.write.option(\"header\", \"true\").csv(output_directory, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67854ad8-6db7-47cc-b63b-38f3f30c9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================================================>(313 + 1) / 314]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|pub_date|\n",
      "+--------+\n",
      "|1953    |\n",
      "|1903    |\n",
      "|1957    |\n",
      "|1897    |\n",
      "|1987    |\n",
      "|1880    |\n",
      "|1956    |\n",
      "|1936    |\n",
      "|2016    |\n",
      "|2020    |\n",
      "|2012    |\n",
      "|1958    |\n",
      "|1910    |\n",
      "|1943    |\n",
      "|1915    |\n",
      "|1972    |\n",
      "|1931    |\n",
      "|1938    |\n",
      "|1926    |\n",
      "|1988    |\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get distinct publication dates\n",
    "unique_pub_dates = df.select(\"pub_date\").distinct()\n",
    "\n",
    "# Show the unique publication dates\n",
    "unique_pub_dates.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8311eed2-ec0a-43c2-8234-69e301e74788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|null| 2439|\n",
      "|1512|    1|\n",
      "|1799|   20|\n",
      "|1800|   38|\n",
      "|1801|   27|\n",
      "|1802|   21|\n",
      "|1803|   44|\n",
      "|1804|   13|\n",
      "|1805|   33|\n",
      "|1806|   21|\n",
      "|1807|   21|\n",
      "|1808|   24|\n",
      "|1809|   23|\n",
      "|1810|   21|\n",
      "|1811|   28|\n",
      "|1812|   29|\n",
      "|1813|   14|\n",
      "|1814|   30|\n",
      "|1815|   27|\n",
      "|1816|   22|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# Assuming 'pub_date' is a valid date column; if not, adjust the parsing logic accordingly\n",
    "df = df.withColumn(\"year\", year(col(\"pub_date\")))\n",
    "\n",
    "# Aggregate publication counts by year\n",
    "yearly_counts = df.groupBy(\"year\").count().orderBy(\"year\")\n",
    "yearly_counts.show()\n",
    "yearly_data = yearly_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaaded6-029c-48d3-9d22-41564d868536",
   "metadata": {},
   "source": [
    "### Non ci sono file vuoti, adesso andiamo a creare i csv divisi per anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07af74ff-da38-48a4-acc2-df710ecc6757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Input directory containing all CSV files\n",
    "input_directory = \"/share/smartdata/citations/update_date\"\n",
    "\n",
    "# Output directory for CSV files organized by publication year\n",
    "output_directory = \"/share/smartdata/citations/update_year_csv\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Read all CSV files in the input directory into a single DataFrame\n",
    "all_data_df = spark.read.option(\"header\", \"true\").csv(input_directory)\n",
    "\n",
    "# Assuming 'pub_date' contains the year and possibly more details, we need to extract just the year\n",
    "# Here we add a new column 'year' that extracts the year from 'pub_date'\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "all_data_df = all_data_df.withColumn(\"year\", regexp_extract(\"pub_date\", r\"(\\d{4})\", 1))\n",
    "\n",
    "# Now, write out the data, partitioned by the 'year' column\n",
    "(all_data_df.write\n",
    "    .partitionBy(\"year\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(output_directory, mode=\"overwrite\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec75c5c-4fd4-4394-b5bc-dce1d60f250b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-year folder: year=__HIVE_DEFAULT_PARTITION__\n",
      "Number of directories with years more than 2024: 1\n",
      "Number of directories with years less or equal than 2024: 227\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Directory containing CSV files organized by year\n",
    "output_directory = \"/share/smartdata/citations/update_year_csv\"\n",
    "\n",
    "# Initialize counters\n",
    "count = 0\n",
    "count_valid_csv = 0\n",
    "\n",
    "# Iterate over each folder in the directory\n",
    "for folder in os.listdir(output_directory):\n",
    "    if folder.startswith(\"year=\"):\n",
    "        # Try to extract and convert the year part of the folder name\n",
    "        try:\n",
    "            year = int(folder.split('=')[1])  # Extract the year after 'year='\n",
    "            if year > 2024:\n",
    "                count += 1\n",
    "            else:\n",
    "                count_valid_csv += 1\n",
    "        except ValueError:\n",
    "            # Skip the folder if the year part is not an integer\n",
    "            print(f\"Skipping non-year folder: {folder}\")\n",
    "\n",
    "print(f\"Number of directories with years more than 2024: {count}\")\n",
    "print(f\"Number of directories with years less or equal than 2024: {count_valid_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87fbbe30-af70-468d-b1d1-efd5a47a1522",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:====================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid rows count: 2471\n",
      "Valid rows count: 123124347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "base_path = \"/share/smartdata/citations/update_year_csv\"\n",
    "\n",
    "# Read the data from CSV files\n",
    "df = spark.read.option(\"header\", \"true\").csv(base_path + \"/*/part-*.csv\", inferSchema=True)\n",
    "df = df.withColumn(\"file_path\", input_file_name())  # Add file path to each row\n",
    "df = df.repartition(200)  # Repartition the DataFrame to manage memory more effectively\n",
    "df = df.persist(StorageLevel.MEMORY_AND_DISK)  # Persist the DataFrame in memory and disk\n",
    "\n",
    "# Extract the year from the file path and process data\n",
    "df = df.withColumn(\"year\", regexp_extract(\"file_path\", \"year=(\\d+|__HIVE_DEFAULT_PARTITION__)\", 1))\n",
    "invalid_df = df.filter((df[\"year\"] > \"2024\") | (df[\"year\"] == \"__HIVE_DEFAULT_PARTITION__\"))\n",
    "valid_df = df.filter(df[\"year\"] <= \"2024\")\n",
    "\n",
    "# Count the invalid and valid rows\n",
    "invalid_rows = invalid_df.count()\n",
    "valid_rows = valid_df.count()\n",
    "\n",
    "# Output the counts\n",
    "print(\"Invalid rows count:\", invalid_rows)\n",
    "print(\"Valid rows count:\", valid_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fca9c-65ec-4b17-9583-30043c650396",
   "metadata": {},
   "source": [
    "### Salvo i dataset validi in una cartella, quelli non validi in un'altra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53522eca-c81b-449e-969d-51df4b26bd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "input_directory = \"/share/smartdata/citations/update_year_csv\"\n",
    "\n",
    "# Output directories for partitioned CSV files\n",
    "output_directory_valid = \"/share/smartdata/citations/UPDATED_VALID_YEAR_DF\"\n",
    "output_directory_invalid = \"/share/smartdata/citations/UPDATED_UNVALID_YEAR_DF\"\n",
    "\n",
    "# Create the output directories if they do not exist\n",
    "os.makedirs(output_directory_valid, exist_ok=True)\n",
    "os.makedirs(output_directory_invalid, exist_ok=True)\n",
    "\n",
    "# Read all CSV files in the input directory into a single DataFrame\n",
    "all_data_df = spark.read.option(\"header\", \"true\").csv(input_directory)\n",
    "\n",
    "# Assuming 'pub_date' contains the year and possibly more details, we need to extract just the year\n",
    "# Adding a new column 'year' that extracts the year from 'pub_date'\n",
    "all_data_df = all_data_df.withColumn(\"year\", regexp_extract(\"pub_date\", r\"(\\d{4})\", 1))\n",
    "\n",
    "# Filtering data into valid and invalid DataFrames based on the year\n",
    "valid_data_df = all_data_df.filter((col(\"year\") <= \"2024\") & (col(\"year\").isNotNull()))\n",
    "invalid_data_df = all_data_df.filter((col(\"year\") > \"2024\") | (col(\"year\").isNull()))\n",
    "\n",
    "# Writing out the valid and invalid data, partitioned by the 'year' column\n",
    "valid_data_df.write.partitionBy(\"year\").option(\"header\", \"true\").csv(output_directory_valid, mode=\"overwrite\")\n",
    "invalid_data_df.write.option(\"header\", \"true\").csv(output_directory_invalid, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39bf60f-e581-4d38-ad73-36c52d2a5ecb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/02 19:19:37 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "# Chiusura della sessione Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252df86-08e4-4d18-b498-747e2ccb5396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

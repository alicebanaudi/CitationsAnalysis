{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd43a121-7fbf-4fbc-a6e7-d8824885201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/22 07:35:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ISSN_ISBN_Comparison\") \\\n",
    "    .config(\"spark.executor.instances\", \"5\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"<your-spark-image>\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16346e6-3548-4d24-ae6c-738f5766bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the validation DataFrame (issn_isbn.csv)\n",
    "issn_isbn_df = spark.read.option(\"header\", \"true\").csv('/share/smartdata/citations/issn_isbn.csv', inferSchema=True)\n",
    "\n",
    "# Load the input DataFrame (checkpoint_4_processed/*.csv)\n",
    "input_files_df = spark.read.option(\"header\", \"true\").csv('/share/smartdata/citations/checkpoint_4_processed/*.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64df650-b08c-4aac-b3dd-2781d7507313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation successful\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"/share/smartdata/citations/checkpoint_spark_issn\"\n",
    "\n",
    "# Use a left semi join to keep only rows with \"issn/isbn\" that exist in the issn_isbn.csv\n",
    "filtered_df = input_files_df.join(\n",
    "    issn_isbn_df,\n",
    "    input_files_df[\"issn/isbn\"] == issn_isbn_df[\"Identifier\"],\n",
    "    \"left_semi\"\n",
    ")\n",
    "\n",
    "# Save the filtered DataFrame back to a directory (optional)\n",
    "filtered_df.write.option(\"header\", \"true\").csv(output_folder, mode='overwrite')\n",
    "print(\"Operation successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0faa6987-ea9f-4eba-b805-bd36f5bda1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- pub_date: string (nullable = true)\n",
      " |-- issn/isbn: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+\n",
      "|                  id|               title|              author|               venue|pub_date|issn/isbn|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|C M Giles [omid:r...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|T Gedde-Dahl [omi...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|E B Robson [omid:...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|E Thorsby [omid:r...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|B Olaisen [omid:r...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|A Arnason [omid:r...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|F Kissmeyer-Niels...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|I Schreuder [omid...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Inhibition Of B C...|A J D'Apice [omid...|Tissue Antigens [...|    1977|0001-2815|\n",
      "|omid:br/066040172...|Inhibition Of B C...|B D Tait [omid:ra...|Tissue Antigens [...|    1977|0001-2815|\n",
      "|omid:br/066040172...|Inhibition Of B C...|G Lambert [omid:r...|Tissue Antigens [...|    1977|0001-2815|\n",
      "|omid:br/066040172...|\"A New Homozygous...|B K Jakobsen [omi...|Tissue Antigens [...|    1986|0001-2815|\n",
      "|omid:br/066040172...|\"A New Homozygous...|P Platz [omid:ra/...|Tissue Antigens [...|    1986|0001-2815|\n",
      "|omid:br/066040172...|\"A New Homozygous...|L P Ryder [omid:r...|Tissue Antigens [...|    1986|0001-2815|\n",
      "|omid:br/066040172...|\"A New Homozygous...|A Svejgaard [omid...|Tissue Antigens [...|    1986|0001-2815|\n",
      "|omid:br/066040172...|Behçet Disease An...|M Takano [omid:ra...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Behçet Disease An...|T Miyajima [omid:...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Behçet Disease An...|M Kiuchi [omid:ra...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Behçet Disease An...|K Ohmori [omid:ra...|Tissue Antigens [...|    1976|0001-2815|\n",
      "|omid:br/066040172...|Behçet Disease An...|H Amemiya [omid:r...|Tissue Antigens [...|    1976|0001-2815|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/share/smartdata/citations/checkpoint_spark_issn/part-00162-d327f767-41cb-4d9f-b9c8-223d67285f9b-c000.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5670a859-efe0-43ce-a0f2-b3c364406cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+\n",
      "|                  id|               title|              author|               venue|pub_date|issn/isbn|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+\n",
      "|omid:br/066040172...|Rga (Rodgers) And...|C M Giles [omid:r...|Tissue Antigens [...|    1976|0001-2815|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bec12c-51b3-4fc9-91ed-48eab5ff1fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: omid:br/06604017286 pmid:968900\n",
      "Title: Rga (Rodgers) And The HLA Region: Linkage And Associations.\n",
      "Author: C M Giles [omid:ra/066010948469]\n",
      "Venue: Tissue Antigens [omid:br/0614049880 issn:1399-0039 issn:0001-2815]\n",
      "Publication Date: 1976\n",
      "ISSN/ISBN: 0001-2815\n"
     ]
    }
   ],
   "source": [
    "# Get the first row of the DataFrame\n",
    "first_row = df.first()\n",
    "\n",
    "# Print each value from the first row in separate outputs\n",
    "print(\"ID:\", first_row['id'])\n",
    "print(\"Title:\", first_row['title'])\n",
    "print(\"Author:\", first_row['author'])\n",
    "print(\"Venue:\", first_row['venue'])\n",
    "print(\"Publication Date:\", first_row['pub_date'])\n",
    "print(\"ISSN/ISBN:\", first_row['issn/isbn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d67d23-69e8-454f-8e33-5a6aef2b6714",
   "metadata": {},
   "source": [
    "### Ora che ho eliminato tutti i valori per cui i miei issn/isbn non combaciassero, quindi ho solo i venues che mi interessano, conto quante righe ci sono in totale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17f2113-f7fe-4ffe-aec2-7b93b43d9283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================================================> (306 + 8) / 314]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows across all datasets is: 123126818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Directory containing all CSV files\n",
    "data_directory = \"/share/smartdata/citations/checkpoint_spark_issn\"\n",
    "\n",
    "# Read all CSV files in the directory into a single DataFrame\n",
    "all_data_df = spark.read.option(\"header\", \"true\").csv(data_directory)\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "row_count = all_data_df.count()\n",
    "\n",
    "# Print the result\n",
    "print(f\"The total number of rows across all datasets is: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b4f95-4cfb-4e7b-9f48-e9919bc50267",
   "metadata": {},
   "source": [
    "### Eliminiamo i csv vuoti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a149f594-7fd2-4500-b811-a94bc65e5126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty files identified (not deleted): []\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# List all CSV files in the directory using glob\n",
    "files = glob.glob(os.path.join(data_directory, '*.csv'))\n",
    "\n",
    "# Check each file and delete if empty\n",
    "empty_files = []\n",
    "for file_path in files:\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "    if df.count() == 0:  # Check if the DataFrame is empty\n",
    "        empty_files.append(file_path)\n",
    "        # Uncomment the next line to delete the file. Be cautious with deletion operations.\n",
    "        #os.remove(file_path)\n",
    "        print(f\"Deleted empty file: {file_path}\")\n",
    "    #else:\n",
    "        #print(f\"Checked and retained: {file_path}\")\n",
    "\n",
    "# Optionally, print all identified empty files (for review before actual deletion)\n",
    "print(\"Empty files identified (not deleted):\", empty_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3f82a-b919-468a-bd44-b57f93e474e1",
   "metadata": {},
   "source": [
    "### Modifico il formato di pub_date rendendolo uniforme per tutti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e917d976-515f-442e-b7fe-08c61374a575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_directory = \"/share/smartdata/citations/checkpoint_4_processed\"\n",
    "# Read all CSV files in the directory into a single DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(data_directory + \"/*.csv\")\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Extract the year from the 'pub_date' and overwrite the 'pub_date' column\n",
    "df = df.withColumn(\"pub_date\", regexp_extract(col(\"pub_date\"), r\"^(\\d{4})\", 1))\n",
    "\n",
    "# Specify the output directory for the modified data\n",
    "output_directory = \"/share/smartdata/citations/modified_pub_dates\"\n",
    "df.write.option(\"header\", \"true\").csv(output_directory, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67854ad8-6db7-47cc-b63b-38f3f30c9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|pub_date|\n",
      "+--------+\n",
      "|1897    |\n",
      "|1957    |\n",
      "|1987    |\n",
      "|2016    |\n",
      "|2012    |\n",
      "|2020    |\n",
      "|1870    |\n",
      "|1953    |\n",
      "|1903    |\n",
      "|1956    |\n",
      "|1936    |\n",
      "|1958    |\n",
      "|1880    |\n",
      "|1808    |\n",
      "|1500    |\n",
      "|1791    |\n",
      "|1686    |\n",
      "|1706    |\n",
      "|1773    |\n",
      "|1669    |\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get distinct publication dates\n",
    "unique_pub_dates = df.select(\"pub_date\").distinct()\n",
    "\n",
    "# Show the unique publication dates\n",
    "unique_pub_dates.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8311eed2-ec0a-43c2-8234-69e301e74788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|year| count|\n",
      "+----+------+\n",
      "|null|110825|\n",
      "|1500|   196|\n",
      "|1508|     1|\n",
      "|1509|     1|\n",
      "|1512|     2|\n",
      "|1515|     1|\n",
      "|1526|     1|\n",
      "|1545|     1|\n",
      "|1566|     1|\n",
      "|1568|     2|\n",
      "|1573|     2|\n",
      "|1578|     1|\n",
      "|1579|     1|\n",
      "|1580|     3|\n",
      "|1581|     1|\n",
      "|1582|     1|\n",
      "|1584|     2|\n",
      "|1588|     1|\n",
      "|1589|     2|\n",
      "|1590|     6|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# Assuming 'pub_date' is a valid date column; if not, adjust the parsing logic accordingly\n",
    "df = df.withColumn(\"year\", year(col(\"pub_date\")))\n",
    "\n",
    "# Aggregate publication counts by year\n",
    "yearly_counts = df.groupBy(\"year\").count().orderBy(\"year\")\n",
    "yearly_counts.show()\n",
    "yearly_data = yearly_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaaded6-029c-48d3-9d22-41564d868536",
   "metadata": {},
   "source": [
    "### Non ci sono file vuoti, adesso andiamo a creare i csv divisi per anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38897903-4bad-4020-96cc-2b7329ac22e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/22 15:35:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"Organize Data by Year\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07af74ff-da38-48a4-acc2-df710ecc6757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Input directory containing all CSV files\n",
    "input_directory = \"/share/smartdata/citations/modified_pub_dates\"\n",
    "\n",
    "# Output directory for CSV files organized by publication year\n",
    "output_directory = \"/share/smartdata/citations/spark_year_csv\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Read all CSV files in the input directory into a single DataFrame\n",
    "all_data_df = spark.read.option(\"header\", \"true\").csv(input_directory)\n",
    "\n",
    "# Assuming 'pub_date' contains the year and possibly more details, we need to extract just the year\n",
    "# Here we add a new column 'year' that extracts the year from 'pub_date'\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "all_data_df = all_data_df.withColumn(\"year\", regexp_extract(\"pub_date\", r\"(\\d{4})\", 1))\n",
    "\n",
    "# Now, write out the data, partitioned by the 'year' column\n",
    "(all_data_df.write\n",
    "    .partitionBy(\"year\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(output_directory, mode=\"overwrite\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec75c5c-4fd4-4394-b5bc-dce1d60f250b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-year folder: year=__HIVE_DEFAULT_PARTITION__\n",
      "Number of directories with years more than 2024: 28\n",
      "Number of directories with years less or equal than 2024: 439\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Directory containing CSV files organized by year\n",
    "output_directory = \"/share/smartdata/citations/spark_year_csv\"\n",
    "\n",
    "# Initialize counters\n",
    "count = 0\n",
    "count_valid_csv = 0\n",
    "\n",
    "# Iterate over each folder in the directory\n",
    "for folder in os.listdir(output_directory):\n",
    "    if folder.startswith(\"year=\"):\n",
    "        # Try to extract and convert the year part of the folder name\n",
    "        try:\n",
    "            year = int(folder.split('=')[1])  # Extract the year after 'year='\n",
    "            if year > 2024:\n",
    "                count += 1\n",
    "            else:\n",
    "                count_valid_csv += 1\n",
    "        except ValueError:\n",
    "            # Skip the folder if the year part is not an integer\n",
    "            print(f\"Skipping non-year folder: {folder}\")\n",
    "\n",
    "print(f\"Number of directories with years more than 2024: {count}\")\n",
    "print(f\"Number of directories with years less or equal than 2024: {count_valid_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3782f8cd-156e-473e-8bc0-04262a7513c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/23 16:04:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Your App Name\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87fbbe30-af70-468d-b1d1-efd5a47a1522",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/23 16:05:01 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid rows count: 111789\n",
      "Valid rows count: 536284096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_15 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_60 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_19 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_16 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_4 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_42 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_146 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_116 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_167 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_78 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_165 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_71 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_89 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_108 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_76 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_35 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_102 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_131 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_101 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_132 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_125 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_48 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_90 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_8 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_38 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_112 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_23 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_30 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_160 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_0 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_109 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_47 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_156 !\n",
      "24/08/23 16:16:18 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_107 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_128 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_46 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_100 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_110 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_162 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_150 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_32 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_123 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_54 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_154 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_114 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_135 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_164 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_121 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_43 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_6 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_139 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_74 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_147 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_36 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_124 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_81 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_106 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_72 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_61 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_59 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_57 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_1 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_14 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_34 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_118 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_153 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_105 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_134 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_151 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_117 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_113 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_129 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_138 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_104 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_56 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_103 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_130 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_163 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_40 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_91 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_111 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_88 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_79 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_25 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_7 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_3 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_37 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_33 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_69 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_80 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_120 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_17 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_53 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_63 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_65 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_94 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_55 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_10 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_140 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_119 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_85 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_28 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_27 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_84 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_20 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_12 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_51 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_66 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_50 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_148 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_22 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_161 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_83 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_5 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_44 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_18 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_127 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_152 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_92 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_2 !\n",
      "24/08/23 16:16:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_45 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_157 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_70 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_52 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_21 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_41 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_75 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_49 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_11 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_115 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_171 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_93 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_158 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_64 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_68 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_95 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_174 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_62 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_145 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_144 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_98 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_168 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_73 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_183 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_188 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_126 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_122 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_184 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_197 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_86 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_196 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_142 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_155 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_172 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_13 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_195 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_77 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_182 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_191 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_133 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_141 !\n",
      "24/08/23 16:16:20 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_170 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_187 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_9 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_159 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_173 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_31 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_179 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_193 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_136 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_190 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_143 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_39 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_181 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_185 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_67 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_194 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_149 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_87 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_186 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_137 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_178 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_199 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_82 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_169 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_166 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_176 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_29 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_175 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_96 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_177 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_26 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_180 !\n",
      "24/08/23 16:16:21 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_58 !\n",
      "24/08/23 16:16:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_192 !\n",
      "24/08/23 16:16:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_24 !\n",
      "24/08/23 16:16:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_99 !\n",
      "24/08/23 16:16:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_23_189 !\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "base_path = \"/share/smartdata/citations/spark_year_csv\"\n",
    "\n",
    "# Read the data from CSV files\n",
    "df = spark.read.option(\"header\", \"true\").csv(base_path + \"/*/part-*.csv\", inferSchema=True)\n",
    "df = df.withColumn(\"file_path\", input_file_name())  # Add file path to each row\n",
    "df = df.repartition(200)  # Repartition the DataFrame to manage memory more effectively\n",
    "df = df.persist(StorageLevel.MEMORY_AND_DISK)  # Persist the DataFrame in memory and disk\n",
    "\n",
    "# Extract the year from the file path and process data\n",
    "df = df.withColumn(\"year\", regexp_extract(\"file_path\", \"year=(\\d+|__HIVE_DEFAULT_PARTITION__)\", 1))\n",
    "invalid_df = df.filter((df[\"year\"] > \"2024\") | (df[\"year\"] == \"__HIVE_DEFAULT_PARTITION__\"))\n",
    "valid_df = df.filter(df[\"year\"] <= \"2024\")\n",
    "\n",
    "# Count the invalid and valid rows\n",
    "invalid_rows = invalid_df.count()\n",
    "valid_rows = valid_df.count()\n",
    "\n",
    "# Output the counts\n",
    "print(\"Invalid rows count:\", invalid_rows)\n",
    "print(\"Valid rows count:\", valid_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fca9c-65ec-4b17-9583-30043c650396",
   "metadata": {},
   "source": [
    "### Salvo i dataset validi in una cartella, quelli non validi in un'altra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41dd28f-04b9-403e-8698-b4c9026c29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/23 16:57:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Partition Data by Year\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53522eca-c81b-449e-969d-51df4b26bd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/23 16:57:47 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "input_directory = \"/share/smartdata/citations/spark_year_csv\"\n",
    "\n",
    "# Output directories for partitioned CSV files\n",
    "output_directory_valid = \"/share/smartdata/citations/spark_year_csv/VALID_YEAR_DF\"\n",
    "output_directory_invalid = \"/share/smartdata/citations/spark_year_csv/UNVALID_YEAR_DF\"\n",
    "\n",
    "# Create the output directories if they do not exist\n",
    "os.makedirs(output_directory_valid, exist_ok=True)\n",
    "os.makedirs(output_directory_invalid, exist_ok=True)\n",
    "\n",
    "# Read all CSV files in the input directory into a single DataFrame\n",
    "all_data_df = spark.read.option(\"header\", \"true\").csv(input_directory)\n",
    "\n",
    "# Assuming 'pub_date' contains the year and possibly more details, we need to extract just the year\n",
    "# Adding a new column 'year' that extracts the year from 'pub_date'\n",
    "all_data_df = all_data_df.withColumn(\"year\", regexp_extract(\"pub_date\", r\"(\\d{4})\", 1))\n",
    "\n",
    "# Filtering data into valid and invalid DataFrames based on the year\n",
    "valid_data_df = all_data_df.filter((col(\"year\") <= \"2024\") & (col(\"year\").isNotNull()))\n",
    "invalid_data_df = all_data_df.filter((col(\"year\") > \"2024\") | (col(\"year\").isNull()))\n",
    "\n",
    "# Writing out the valid and invalid data, partitioned by the 'year' column\n",
    "valid_data_df.write.partitionBy(\"year\").option(\"header\", \"true\").csv(output_directory_valid, mode=\"overwrite\")\n",
    "invalid_data_df.write.option(\"header\", \"true\").csv(output_directory_invalid, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a11c7f-2634-408f-a5c6-a6a910659204",
   "metadata": {},
   "source": [
    "### Ha salvato in modo corretto valid_year_df (ovvero mantenendo le sottocartelle), mentre per unvalid no, quindi adesso eliminerò e risalverò unvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848e563f-52c5-4c4a-8eeb-625bdc32c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Input directory containing all CSV files\n",
    "input_directory = \"/share/smartdata/citations/spark_year_csv\"\n",
    "\n",
    "# Exclude the VALID_YEAR_DF directory by not listing it in the read operation\n",
    "input_glob_path = input_directory + \"/year=*/[!V]*\"\n",
    "\n",
    "# Output directory for invalid year data\n",
    "output_directory_invalid = \"/share/smartdata/citations/spark_year_csv/UNVALID_YEAR_DF\"\n",
    "\n",
    "# Read all CSV files in the input directory into a single DataFrame\n",
    "# Ensure the DataFrame understands it needs to look for partitioned data\n",
    "all_data_df = spark.read.option(\"header\", \"true\").csv(input_glob_path)\n",
    "\n",
    "# Assuming 'pub_date' contains the year and possibly more details, we need to extract just the year\n",
    "# Adding a new column 'year' that extracts the year from 'pub_date'\n",
    "all_data_df = all_data_df.withColumn(\"year\", regexp_extract(\"pub_date\", r\"(\\d{4})\", 1))\n",
    "\n",
    "# Filtering data into an invalid DataFrame based on the year\n",
    "invalid_data_df = all_data_df.filter((col(\"year\") > \"2024\") | (col(\"year\").isNull()))\n",
    "\n",
    "# Writing out the invalid data, partitioned by the 'year' column\n",
    "# This will create subfolders for each 'year' in the UNVALID_YEAR_DF directory\n",
    "invalid_data_df.write.partitionBy(\"year\").option(\"header\", \"true\").csv(output_directory_invalid, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b64600-dff2-491d-af45-7102d76792c8",
   "metadata": {},
   "source": [
    "### Voglio sapere quante righe ci sono in questa cartella per farmi un'idea della perdita di informazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf0c950-aa02-4d43-8dc2-6e020ba0477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:====================================================>    (52 + 5) / 57]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the HIVE default partition: 110825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the directory path for the HIVE default partition\n",
    "hive_default_partition_path = \"/share/smartdata/citations/spark_year_csv/year=__HIVE_DEFAULT_PARTITION__\"\n",
    "\n",
    "# Read all CSV files from the HIVE default partition directory\n",
    "default_partition_df = spark.read.option(\"header\", \"true\").csv(hive_default_partition_path)\n",
    "\n",
    "# Count the number of rows in this partition\n",
    "row_count = default_partition_df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"Number of rows in the HIVE default partition:\", row_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39bf60f-e581-4d38-ad73-36c52d2a5ecb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/12 13:20:54 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "# Chiusura della sessione Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252df86-08e4-4d18-b498-747e2ccb5396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
